{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controller-Based Ghost Clipping: Memory-Efficient DP Training\n",
    "\n",
    "This tutorial demonstrates how to use **Ghost Clipping** with the controller-based privacy engine. Ghost Clipping provides significant memory savings by computing per-sample gradient *norms* directly without materializing full per-sample gradients.\n",
    "\n",
    "## Why Ghost Clipping + Controller?\n",
    "\n",
    "Combining these two approaches gives you:\n",
    "- **Memory Efficiency**: Ghost clipping avoids storing full per-sample gradients\n",
    "- **No Model Wrapping**: Controller-based approach preserves model type\n",
    "- **Transformer Compatibility**: Perfect for large models like BERT, GPT, etc.\n",
    "- **Faster Training**: Reduced memory footprint allows larger batch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from opacus.privacy_engine_gsc import PrivacyEngineGradSampleController\n",
    "from opacus.utils.fast_gradient_clipping_utils import DPLossFastGradientClipping\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a synthetic dataset\n",
    "n_samples = 1000\n",
    "n_features = 50\n",
    "n_classes = 10\n",
    "\n",
    "X = torch.randn(n_samples, n_features)\n",
    "y = torch.randint(0, n_classes, (n_samples,))\n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepClassifier(nn.Module):\n",
    "    \"\"\"Deeper model to demonstrate memory savings\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.layer_norm(self.relu(self.fc2(x)))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = DeepClassifier(n_features, 256, n_classes)\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Controller-Based Training (without Ghost Clipping)\n",
    "\n",
    "First, let's see the standard approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard controller-based approach\n",
    "model_standard = DeepClassifier(n_features, 256, n_classes)\n",
    "optimizer_standard = optim.Adam(model_standard.parameters(), lr=0.001)\n",
    "\n",
    "privacy_engine_standard = PrivacyEngineGradSampleController()\n",
    "\n",
    "model_standard, optimizer_standard, dataloader_standard = privacy_engine_standard.make_private(\n",
    "    module=model_standard,\n",
    "    optimizer=optimizer_standard,\n",
    "    data_loader=dataloader,\n",
    "    noise_multiplier=1.0,\n",
    "    max_grad_norm=1.0,\n",
    "    grad_sample_mode=\"hooks\",  # Standard mode\n",
    ")\n",
    "\n",
    "print(\"Standard controller-based approach configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ghost Clipping with Controller\n",
    "\n",
    "Now let's use ghost clipping for memory efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ghost clipping approach\n",
    "model_ghost = DeepClassifier(n_features, 256, n_classes)\n",
    "optimizer_ghost = optim.Adam(model_ghost.parameters(), lr=0.001)\n",
    "\n",
    "privacy_engine_ghost = PrivacyEngineGradSampleController()\n",
    "\n",
    "# Use grad_sample_mode=\"ghost\" for ghost clipping\n",
    "controller, optimizer_ghost, dataloader_ghost = privacy_engine_ghost.make_private(\n",
    "    module=model_ghost,\n",
    "    optimizer=optimizer_ghost,\n",
    "    data_loader=dataloader,\n",
    "    noise_multiplier=1.0,\n",
    "    max_grad_norm=1.0,\n",
    "    grad_sample_mode=\"ghost\",  # Ghost clipping mode!\n",
    "    return_controller=True,  # Get controller for loss wrapper\n",
    ")\n",
    "\n",
    "print(f\"Ghost clipping configured\")\n",
    "print(f\"Model type preserved: {isinstance(model_ghost, DeepClassifier)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Ghost Clipping\n",
    "\n",
    "Ghost clipping requires a special loss wrapper that performs two backward passes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_ghost = model_ghost.to(device)\n",
    "\n",
    "# Create loss wrapper for ghost clipping\n",
    "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "loss_fn = DPLossFastGradientClipping(\n",
    "    module=controller,\n",
    "    optimizer=optimizer_ghost,\n",
    "    criterion=criterion,\n",
    "    loss_reduction=\"mean\",\n",
    ")\n",
    "\n",
    "EPOCHS = 3\n",
    "DELTA = 1e-5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model_ghost.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(dataloader_ghost):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer_ghost.zero_grad()\n",
    "        output = model_ghost(data)\n",
    "        \n",
    "        # Use loss wrapper (performs two backward passes)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()  # This handles ghost clipping magic\n",
    "        \n",
    "        optimizer_ghost.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    epsilon = privacy_engine_ghost.get_epsilon(DELTA)\n",
    "    avg_loss = total_loss / len(dataloader_ghost)\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} | Loss: {avg_loss:.4f} | ε: {epsilon:.2f} (δ={DELTA})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Ghost Clipping Works\n",
    "\n",
    "Ghost clipping performs **two backward passes**:\n",
    "\n",
    "1. **First Pass**: Computes per-sample gradient norms (without storing full gradients)\n",
    "2. **Compute Clipping Coefficients**: `coeff = min(1, max_grad_norm / norm)`\n",
    "3. **Second Pass**: Backprop with loss scaled by clipping coefficients\n",
    "\n",
    "This avoids storing full per-sample gradients, saving memory!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Comparison\n",
    "\n",
    "Let's visualize the memory difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.cuda as cuda\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Reset memory stats\n",
    "    cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Run one batch with standard approach\n",
    "    data, target = next(iter(dataloader_standard))\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model_standard(data)\n",
    "    loss = nn.functional.cross_entropy(output, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    standard_memory = cuda.max_memory_allocated() / 1024**2  # MB\n",
    "    print(f\"Standard approach peak memory: {standard_memory:.2f} MB\")\n",
    "    \n",
    "    # Reset for ghost clipping\n",
    "    cuda.reset_peak_memory_stats()\n",
    "    \n",
    "    # Run one batch with ghost clipping\n",
    "    data, target = next(iter(dataloader_ghost))\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model_ghost(data)\n",
    "    loss = loss_fn(output, target)\n",
    "    loss.backward()\n",
    "    \n",
    "    ghost_memory = cuda.max_memory_allocated() / 1024**2  # MB\n",
    "    print(f\"Ghost clipping peak memory: {ghost_memory:.2f} MB\")\n",
    "    print(f\"Memory savings: {(1 - ghost_memory/standard_memory)*100:.1f}%\")\n",
    "else:\n",
    "    print(\"GPU not available for memory comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Ghost Clipping?\n",
    "\n",
    "### Use Ghost Clipping When:\n",
    "- Training large models (transformers, ResNets, etc.)\n",
    "- Memory is a bottleneck\n",
    "- You want to use larger batch sizes\n",
    "- Working with Linear and LayerNorm layers (optimized support)\n",
    "\n",
    "### Use Standard Mode When:\n",
    "- Model is small\n",
    "- Memory is not a concern\n",
    "- Model has many custom layers without ghost clipping support\n",
    "- You need per-sample gradients for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ghost Clipping with Target Epsilon\n",
    "\n",
    "You can also use `make_private_with_epsilon`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_epsilon = DeepClassifier(n_features, 256, n_classes)\n",
    "optimizer_epsilon = optim.Adam(model_epsilon.parameters(), lr=0.001)\n",
    "dataloader_epsilon = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "privacy_engine_epsilon = PrivacyEngineGradSampleController()\n",
    "\n",
    "controller_epsilon, optimizer_epsilon, dataloader_epsilon = privacy_engine_epsilon.make_private_with_epsilon(\n",
    "    module=model_epsilon,\n",
    "    optimizer=optimizer_epsilon,\n",
    "    data_loader=dataloader_epsilon,\n",
    "    target_epsilon=3.0,\n",
    "    target_delta=1e-5,\n",
    "    epochs=3,\n",
    "    max_grad_norm=1.0,\n",
    "    grad_sample_mode=\"ghost\",\n",
    "    return_controller=True,\n",
    ")\n",
    "\n",
    "print(f\"Target epsilon: 3.0\")\n",
    "print(f\"Computed noise multiplier: {optimizer_epsilon.noise_multiplier:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Layers\n",
    "\n",
    "Ghost clipping is optimized for:\n",
    "- `nn.Linear`\n",
    "- `nn.LayerNorm`\n",
    "- `nn.Embedding`\n",
    "- And more...\n",
    "\n",
    "For unsupported layers, it automatically falls back to Fast Gradient Clipping (computes full gradients then norms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Remember to cleanup the controller when done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controller.cleanup()\n",
    "print(\"Hooks cleaned up!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Feature | Standard Controller | Ghost Clipping Controller |\n",
    "|---------|--------------------|--------------------------|\n",
    "| Memory Usage | Higher | **Lower** |\n",
    "| Speed | Faster (1 backward) | Slower (2 backwards) |\n",
    "| Model Wrapping | No | **No** |\n",
    "| Type Preservation | Yes | **Yes** |\n",
    "| Best For | Small models | **Large models** |\n",
    "| Batch Size | Smaller | **Larger** |\n",
    "\n",
    "The combination of controller-based approach + ghost clipping is ideal for training large transformers with differential privacy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn More\n",
    "\n",
    "- [Ghost Clipping Paper](https://arxiv.org/abs/2205.09632)\n",
    "- [Controller-Based Tutorial](controller_based_privacy_engine.ipynb)\n",
    "- [Opacus Documentation](https://opacus.ai)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
